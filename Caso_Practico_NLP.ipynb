{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "hih-qcwvsiRe",
    "outputId": "73c9b4a0-1118-431d-9eee-9313e452eae7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\auror\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "ERROR: Invalid requirement: '#'\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\auror\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\auror\\anaconda3\\lib\\site-packages (3.5.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\auror\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\auror\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\auror\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# text processing libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords') # bajar la primera vez\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "# sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,confusion_matrix,balanced_accuracy_score,roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#lime\n",
    "!pip install lime # instalar la primera vez\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "\n",
    "#Lemmatizer\n",
    "nltk.download('wordnet') # bajar la primera vez\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#ngrams\n",
    "from nltk.util import ngrams\n",
    "import collections\n",
    "\n",
    "#spacy\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6079ILvdfQS"
   },
   "source": [
    "FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Fv7eFXj1ddoj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alejandra aaaaamazzzze in my aabzi wooorld coffee aaabzi abzi well\n",
      "final word well\n",
      "final new word well\n",
      "alejandra amaze in my aabzi world coffee abzi abzi well\n",
      "alejandra amaze in my abzi world coffee abzi abzi well\n"
     ]
    }
   ],
   "source": [
    "#Limpieza\n",
    "manual_clean=[] #lista de registros con errores producidos al hacer limpieza\n",
    "def clean_text(i,text):\n",
    " try:\n",
    "    punctuation_list=\"\"\"!#$%&'\"()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text) #remove text in square brackets\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) #remove urls, https\n",
    "    text = re.sub('[%s]' % re.escape(punctuation_list), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) #get rid of digits, ie numbers\n",
    "    \n",
    "    #realizando de nuevo la limpieza \n",
    "    string=re.sub(r'^(\\w)\\1{1,}', r'\\1', text)#letras que se repiten >=2 veces en la 1era palabra\n",
    "    string=re.sub(r'(\\w)\\1{2,}', r'\\1', string)#letras que se repiten más de 2 veces\n",
    "    string=string.strip()\n",
    "    #letras que se repiten dos o más veces en la palabra final\n",
    "    final_word=re.findall(r'(\\w+$)', string)\n",
    "    final_word=final_word[0]\n",
    "    final_new_word=re.sub(r'\\b([a-z]{1})\\1', r'\\1', final_word)\n",
    "    text=re.sub(r\"\\w+$\", final_new_word, string)#se encarga de las letras que se repiten en el resto de la palabra\n",
    "    #letras que se repiten dos veces al inicio de cada palabra\n",
    "    text=re.sub(r'\\b([a-z]{1})\\1', r'\\1', text)\n",
    "    \n",
    " except Exception as error:\n",
    "    manual_clean.append(i) #i es el número de fila, a estos registros les hare limpieza manual\n",
    " else: \n",
    "  return text\n",
    "\n",
    "#este es un ejemplo de lo que hace en la última parte la función anterior\n",
    "string = \"aalejandra aaaaamazzzze in my aabzi wooorld coffee aaabzi abzi well\"\n",
    "string = string.strip()\n",
    "string=re.sub(r'^(\\w)\\1{1,}', r'\\1', string)\n",
    "print(string)\n",
    "string=re.sub(r'(\\w)\\1{2,}', r'\\1', string)\n",
    "final_word=re.findall(r'(\\w+$)', string)\n",
    "final_word=final_word[0]\n",
    "print(\"final word\", final_word)\n",
    "final_new_word=re.sub(r'\\b([a-z]{1})\\1', r'\\1', final_word)\n",
    "print(\"final new word\", final_new_word)\n",
    "saludo=re.sub(r\"\\w+$\", final_new_word, string)\n",
    "print(saludo)\n",
    "string=re.sub(r'\\b([a-z]{1})\\1', r'\\1', string)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6GSNJJVP-J0H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'name', 'rating', 'review', 'bad'], dtype='object')\n",
      "(121814, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"restaurant_ratings.csv\", sep=\",\")\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lUXqT3bOSJry"
   },
   "outputs": [],
   "source": [
    "#eliminamos las columnas name y rating\n",
    "df.drop([\"name\",\"rating\"], axis=1, inplace=True)\n",
    "df.rename(columns={\"Unnamed: 0\":\"id\"}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OZaK1B63WLyI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121814, 3)\n",
      "         id                                             review  bad\n",
      "0    228122  Ambience  35  Too noisy\\nFood  3\\nService  4\\n...  0.0\n",
      "1    400236  If I could  I would rate it in negativeWorst s...  1.0\n",
      "2     35438                                    Hemanth reddyK   0.0\n",
      "3    262748  Try out the filter coffee masala dosa idly and...  0.0\n",
      "4   1008738  Decent Mutton Biryani Consistent I have tried ...  0.0\n",
      "5    422366  Great ambience good food and best service by t...  0.0\n",
      "6    438899  Ambiance  The live music and the quirky decor ...  0.0\n",
      "7    882757  Food tastes really good Very Different Chinese...  0.0\n",
      "8    868560  Extremely happy about my orderthe food was als...  0.0\n",
      "9    614196  didnt get half my order  zomato credited me th...  0.0\n",
      "10  1019204  Ive never been to this place But I have ordere...  0.0\n",
      "11   132396  Update  They now accept cards\\n\\nFood review \\...  0.0\n",
      "12   308568  Soya chaap brings back the memories of old Nor...  0.0\n",
      "13   615715  Have bookmarked this place since it was coming...  0.0\n",
      "14    90601  After fun games at Play Arena when it was time...  0.0\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m6081GwBXpZi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "review    322\n",
       "bad         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.id.duplicated().sum())#no hay duplicados\n",
    "#Analizando el número de nulos\n",
    "df.isnull().sum() #en review tenemos 322 nulos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qHCb5YoYaeGC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    293\n",
       "1.0     29\n",
       "Name: bad, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.review.isnull()].bad.value_counts() #la mayoría de los nulos son del grupo mayoritario de buenas reseñas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H1WR6DniY_FY"
   },
   "outputs": [],
   "source": [
    "df.bad.value_counts() #la mayoría son cero de los que son nulos, los eliminaremos estos\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"index\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LJKnJcgZXRlI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121492, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "#inicio de limpieza\n",
    "df_cleaned=df.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.loc[:,\"review_cleaned\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Lra77ACxFWhk"
   },
   "outputs": [],
   "source": [
    "#aplicando la función de limpieza\n",
    "rows=df.shape[0]\n",
    "for i in range(0,rows):\n",
    " df_cleaned.review_cleaned.iloc[i]=clean_text(i,df.review.iloc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "registros con errores  65\n",
      "0.0    60\n",
      "1.0     5\n",
      "Name: bad, dtype: int64\n",
      "",
      "",
      " \n",
      "\n",
      "",
      "",
      " \n",
      "\n",
      "       \n",
      "\n",
      "6  \n",
      "\n",
      "",
      "",
      "  \n",
      "\n",
      " 1MG5   \n",
      "\n",
      "                        \n",
      "\n",
      "  ",
      "9",
      "",
      " \n",
      "\n",
      "",
      " 2DJ",
      " \n",
      "\n",
      "gr8 \n",
      "\n",
      " 1MG5   \n",
      "\n",
      "httpmercurialpurpleblogspotin201406notgoodenoughalibihtml \n",
      "\n",
      "gr8 \n",
      "\n",
      "        \n",
      "\n",
      "       \n",
      "\n",
      "5 \n",
      "\n",
      "",
      "",
      " \n",
      "\n",
      "",
      " \n",
      "\n",
      " ",
      "   \n",
      "\n",
      "1 \n",
      "\n",
      " ",
      "      \n",
      "\n",
      "",
      "",
      " ",
      "",
      " \n",
      "\n",
      "  \n",
      "\n",
      "",
      "",
      " \n",
      "\n",
      "httpmercurialpurpleblogspotin201406notgoodenoughalibihtml \n",
      "\n",
      "",
      " \n",
      "\n",
      "               \n",
      "\n",
      "           ",
      "                                            \n",
      "\n",
      "                                       ",
      "                                                                             ",
      "                                                     ",
      "           ",
      "                                \n",
      "                                               \n",
      "\n",
      "",
      " 2DJ",
      " \n",
      "\n",
      "2DJ \n",
      "\n",
      "   \n",
      "\n",
      "MG5BGM \n",
      "\n",
      "1MG5 \n",
      "\n",
      "2DJ \n",
      "\n",
      "5 \n",
      "\n",
      "",
      "",
      "  \n",
      "\n",
      "2 \n",
      "\n",
      "47080BGM \n",
      "\n",
      "  \n",
      "\n",
      "",
      "",
      "",
      " ",
      " \n",
      "\n",
      "",
      " \n",
      "\n",
      "1MG5 \n",
      "\n",
      "",
      "",
      " ",
      "",
      " \n",
      "\n",
      " 1MG5   \n",
      "\n",
      " ",
      " ",
      "  \n",
      "\n",
      "",
      "",
      "  \n",
      "\n",
      "2DJ \n",
      "\n",
      "",
      "",
      " ",
      "",
      " \n",
      "\n",
      "10 \n",
      "\n",
      "",
      "",
      "",
      "  \n",
      "\n",
      "",
      "",
      "",
      "  ",
      " \n",
      "\n",
      "",
      "",
      " ",
      "",
      " \n",
      "\n",
      "   \n",
      "\n",
      "                                   \n",
      "\n",
      "               \n",
      "\n",
      "",
      "   \n",
      "\n",
      "",
      "   \n",
      "\n",
      "4 \n",
      "\n",
      "",
      "",
      " ",
      "",
      " \n",
      "\n",
      "gr8 \n",
      "\n",
      "10 \n",
      "\n",
      "  \n",
      "\n",
      "10111000 \n",
      "\n",
      "2DJ \n",
      "\n",
      "3 \n",
      "\n",
      "35 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#observando los registros que proporcionaron error\n",
    "print(\"registros con errores \",len(manual_clean))\n",
    "print(df.bad.iloc[manual_clean].value_counts()) #la mayoria de errores son de buenas reseñas\n",
    "for i in manual_clean:\n",
    " print(df.review.iloc[i],\"\\n\")\n",
    "#luego de revisarlas, se procederá a borrarlas\n",
    "df_cleaned.review_cleaned.drop(manual_clean,axis=0,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>bad</th>\n",
       "      <th>review_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>228122</td>\n",
       "      <td>Ambience  35  Too noisy\\nFood  3\\nService  4\\n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ambience    too noisyfood     the zomato revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>400236</td>\n",
       "      <td>If I could  I would rate it in negativeWorst s...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>if i could  i would rate it in negativeworst s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35438</td>\n",
       "      <td>Hemanth reddyK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hemanth reddyk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262748</td>\n",
       "      <td>Try out the filter coffee masala dosa idly and...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>try out the filter coffee masala dosa idly and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1008738</td>\n",
       "      <td>Decent Mutton Biryani Consistent I have tried ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>decent mutton biryani consistent i have tried ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121487</th>\n",
       "      <td>33166</td>\n",
       "      <td>The prices are high for the Quantity and quali...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>the prices are high for the quantity and quali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121488</th>\n",
       "      <td>642570</td>\n",
       "      <td>Pasta Street has come up with yet another outl...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pasta street has come up with yet another outl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121489</th>\n",
       "      <td>480976</td>\n",
       "      <td>Want to give it 35\\nNice and quiet ambience\\nF...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>want to give it  and quiet ambiencefood was go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121490</th>\n",
       "      <td>34376</td>\n",
       "      <td>will never order from this restaurant I should...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>will never order from this restaurant i should...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121491</th>\n",
       "      <td>868941</td>\n",
       "      <td>poha is nice but upma is quite rough and dry</td>\n",
       "      <td>0.0</td>\n",
       "      <td>poha is nice but upma is quite rough and dry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121492 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                             review  bad  \\\n",
       "0        228122  Ambience  35  Too noisy\\nFood  3\\nService  4\\n...  0.0   \n",
       "1        400236  If I could  I would rate it in negativeWorst s...  1.0   \n",
       "2         35438                                    Hemanth reddyK   0.0   \n",
       "3        262748  Try out the filter coffee masala dosa idly and...  0.0   \n",
       "4       1008738  Decent Mutton Biryani Consistent I have tried ...  0.0   \n",
       "...         ...                                                ...  ...   \n",
       "121487    33166  The prices are high for the Quantity and quali...  1.0   \n",
       "121488   642570  Pasta Street has come up with yet another outl...  0.0   \n",
       "121489   480976  Want to give it 35\\nNice and quiet ambience\\nF...  0.0   \n",
       "121490    34376  will never order from this restaurant I should...  1.0   \n",
       "121491   868941       poha is nice but upma is quite rough and dry  0.0   \n",
       "\n",
       "                                           review_cleaned  \n",
       "0       ambience    too noisyfood     the zomato revie...  \n",
       "1       if i could  i would rate it in negativeworst s...  \n",
       "2                                          hemanth reddyk  \n",
       "3       try out the filter coffee masala dosa idly and...  \n",
       "4       decent mutton biryani consistent i have tried ...  \n",
       "...                                                   ...  \n",
       "121487  the prices are high for the quantity and quali...  \n",
       "121488  pasta street has come up with yet another outl...  \n",
       "121489  want to give it  and quiet ambiencefood was go...  \n",
       "121490  will never order from this restaurant i should...  \n",
       "121491       poha is nice but upma is quite rough and dry  \n",
       "\n",
       "[121492 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.drop(columns=\"index\", inplace=True)\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eo9iBH4trriR"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "APK1NJrNcW7s"
   },
   "outputs": [],
   "source": [
    " df_cleaned[\"review_cleaned_1\"]=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9FPTdjOKlVXj"
   },
   "outputs": [],
   "source": [
    "#limitando a ciertas palabras mediante su etiquetado gramatical, el vocabulario que no \n",
    "#sean pronombres, simbolos, entre otros...\n",
    "for i,n in enumerate(df_cleaned.review_cleaned.tolist()):\n",
    "\n",
    " doc= nlp(str(n))\n",
    " text=[]\n",
    " for token in doc:\n",
    "    if(not token.pos_ in [\"AUX\",\"ADP\",\"PROPN\",\"SYM\",\"NUM\", \"PUNCT\"]):\n",
    "     text.append(token.text)\n",
    " df_cleaned[\"review_cleaned_1\"].iloc[i]=\" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "aFDomyvRCKS9"
   },
   "outputs": [],
   "source": [
    "df_cleaned[\"tokens_cleaned\"]=\"\"\n",
    "df_cleaned[\"lemma\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VBfFqSmopk83"
   },
   "outputs": [],
   "source": [
    "#lista de stopwords\n",
    "eng_stopwords=stopwords.words(\"english\")\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "\n",
    "#Toquenizaciòn,stopword,lemma\n",
    "for i,text in enumerate(df_cleaned.loc[:,\"review_cleaned_1\"].tolist()):\n",
    "  tokens_final=[]\n",
    "  tokens_init=tokenizer.tokenize(text)\n",
    "  tokens_final=[token for token in tokens_init if not token in eng_stopwords] #stopwords\n",
    "  df_cleaned[\"tokens_cleaned\"].iloc[i]=\" \".join(tokens_final)\n",
    "  doc = nlp(df_cleaned[\"tokens_cleaned\"].iloc[i])\n",
    "  df_cleaned[\"lemma\"].iloc[i]=\" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4VPiGSc7Zuij"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_cleaned</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ambience noisyfood zomato review thought visit...</td>\n",
       "      <td>ambience noisyfood zomato review think visit c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rate shawarma ever diced onion spoils entire t...</td>\n",
       "      <td>rate shawarma ever diced onion spoil entire ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>try filter coffee masala dosa idly everything ...</td>\n",
       "      <td>try filter coffee masala dosa idly everything ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>decent consistent tried times saalan badly mad...</td>\n",
       "      <td>decent consistent try times saalan badly madei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121487</th>\n",
       "      <td>prices high quantity quality food unhygienic p...</td>\n",
       "      <td>price high quantity quality food unhygienic pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121488</th>\n",
       "      <td>come yet another outlet time koramangala conve...</td>\n",
       "      <td>come yet another outlet time koramangala conve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121489</th>\n",
       "      <td>want give good loved soup disappointed dessert...</td>\n",
       "      <td>want give good love soup disappoint dessert or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121490</th>\n",
       "      <td>never order restaurant brought icecreams inste...</td>\n",
       "      <td>never order restaurant bring icecream instead ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121491</th>\n",
       "      <td>poha nice upma quite rough dry</td>\n",
       "      <td>poha nice upma quite rough dry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121492 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tokens_cleaned  \\\n",
       "0       ambience noisyfood zomato review thought visit...   \n",
       "1       rate shawarma ever diced onion spoils entire t...   \n",
       "2                                                           \n",
       "3       try filter coffee masala dosa idly everything ...   \n",
       "4       decent consistent tried times saalan badly mad...   \n",
       "...                                                   ...   \n",
       "121487  prices high quantity quality food unhygienic p...   \n",
       "121488  come yet another outlet time koramangala conve...   \n",
       "121489  want give good loved soup disappointed dessert...   \n",
       "121490  never order restaurant brought icecreams inste...   \n",
       "121491                     poha nice upma quite rough dry   \n",
       "\n",
       "                                                    lemma  \n",
       "0       ambience noisyfood zomato review think visit c...  \n",
       "1       rate shawarma ever diced onion spoil entire ta...  \n",
       "2                                                          \n",
       "3       try filter coffee masala dosa idly everything ...  \n",
       "4       decent consistent try times saalan badly madei...  \n",
       "...                                                   ...  \n",
       "121487  price high quantity quality food unhygienic pl...  \n",
       "121488  come yet another outlet time koramangala conve...  \n",
       "121489  want give good love soup disappoint dessert or...  \n",
       "121490  never order restaurant bring icecream instead ...  \n",
       "121491                     poha nice upma quite rough dry  \n",
       "\n",
       "[121492 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_cleaned[[\"tokens_cleaned\",\"lemma\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "GvrYUN0o1Oxc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.       , 0.       , ..., 0.       , 0.3161062,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       ...,\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf-idf\n",
    "tfdif= TfidfVectorizer(max_features=1500,\n",
    "                        min_df=2,\n",
    "                        max_df=0.5,\n",
    "                        ngram_range=(1, 2))\n",
    "train_tfidf = tfdif.fit_transform(df_cleaned.lemma)\n",
    "train_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "cpp1gaK_VpYd"
   },
   "outputs": [],
   "source": [
    "tfidf_df=pd.DataFrame(train_tfidf.toarray(),\n",
    "             columns=tfdif.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['able', 'absolute', 'absolutely', 'absolutely love', 'accept',\n",
       "       'accommodate', 'accompany', 'accord', 'actually', 'add',\n",
       "       ...\n",
       "       'year', 'yes', 'yesterday', 'yet', 'young', 'yum', 'yumm', 'yummy',\n",
       "       'zomato', 'zomato gold'],\n",
       "      dtype='object', length=1500)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "lADMngua34_9"
   },
   "outputs": [],
   "source": [
    "#dividiendo los datos de entrenamiento y test\n",
    "x=train_tfidf\n",
    "y=df_cleaned.bad\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [],
    "id": "GDjUqS1cwu1M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9242756804214223\n",
      "Precision: 0.8452197378565921\n",
      "Recall: 0.6914222642699464\n",
      "F1: 0.7606244579358195\n"
     ]
    }
   ],
   "source": [
    "#fitting y métricas del modelo Naive Bayes\n",
    "model = LogisticRegression()\n",
    "model = model.fit(x_train, y_train)\n",
    "y_pred_logistic = model.predict(x_test)\n",
    "print(\"Accuracy: \"+str(accuracy_score(y_test,y_pred_logistic)))\n",
    "print(\"Precision: \"+str(precision_score(y_test,y_pred_logistic)))\n",
    "print(\"Recall: \"+str(recall_score(y_test,y_pred_logistic)))\n",
    "print(\"F1: \"+str(f1_score(y_test,y_pred_logistic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gd4LJKzyvPbU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_FuOAatJH4S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIKQZ_-DqYWz"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFac9FBPTO5o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azDLc921Ujcc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pj2wLAIqs2J2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zg3R0UTN1iDL"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
